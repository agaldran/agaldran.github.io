<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Adrian Galdran - Research Site</title>
    <link>https://agaldran.github.io/post/</link>
    <description>Recent content in Posts on Adrian Galdran - Research Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Adrian Galdran</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>DACO - Practical Lecture 1 - Introduction to Python for Scientific Computing</title>
      <link>https://agaldran.github.io/post/17_daco_prac_lec_1/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0100</pubDate>
      
      <guid>https://agaldran.github.io/post/17_daco_prac_lec_1/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;NOTE: pdf slides for the first part of this lecture can be found &lt;a href=&#34;https://github.com/agaldran/daco_2017_practicals/blob/master/lecture_1_python/daco17_practical_lec_1.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This year we have decided to move from Matlab to Python for the practical sessions.
Some of you maybe will not have worked with this programming language.
This first lecture is intended to guide you through your first steps in this programming language,
and make you aware of the (super-rich) Python ecosystem for scientific computing.&lt;/p&gt;

&lt;p&gt;I really hope that by the end of this course you will be a Python fan, and consider abandoning Matlab once and forever!
This is an overview of what you will be learning today:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Motivation and Goals. What is Python?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python Installation. Accompanying Tools&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Anaconda Python Distribution&lt;/li&gt;
&lt;li&gt;Executing Python code&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Introduction to the Python Programming Language&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fundamental Python data types&lt;/li&gt;
&lt;li&gt;Everything is an object&lt;/li&gt;
&lt;li&gt;Python Modules&lt;/li&gt;
&lt;li&gt;Python Basic Operators&lt;/li&gt;
&lt;li&gt;Sequences and Object Containers&lt;/li&gt;
&lt;li&gt;Python typing&lt;/li&gt;
&lt;li&gt;Flow Control&lt;/li&gt;
&lt;li&gt;Python Functions&lt;/li&gt;
&lt;li&gt;Classes and Object-Oriented Programming
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complementary Python Scientific Computing Tools: Numpy&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Introduction to NumPy Arrays&lt;/li&gt;
&lt;li&gt;Slicing NumPy Arrays&lt;/li&gt;
&lt;li&gt;Indexing NumPy Arrays&lt;/li&gt;
&lt;li&gt;Other numpy arrays manipulation techniques&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Complementary Python Scientific Computing Tools: Matplotlib&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Basic Matplotlib usage&lt;/li&gt;
&lt;li&gt;Plot Customization&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Homework ðŸ˜±&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sources and References&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So&amp;hellip; let&amp;rsquo;s move on.&lt;/p&gt;

&lt;h2 id=&#34;1-motivation-and-goals-what-is-python&#34;&gt;1.- &lt;strong&gt;Motivation and Goals. What is Python?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;First thing, Python is &lt;strong&gt;free&lt;/strong&gt;. Second, it is &lt;strong&gt;simple&lt;/strong&gt;.
Third, it is increasingly becoming the tool of choice for data science projects.
Fourth, it&amp;rsquo;s multi-platform, it can run in Windows, Linux, Mac, your mobile phone&amp;hellip;
And last, there is a &lt;strong&gt;huge&lt;/strong&gt; community of contributors to lots of open-source projects that complement it.
This manifests in the form of a large ecosystem of scientific computing tools that grow along with the number of users.&lt;/p&gt;

&lt;p&gt;However, to add all this to your tool-belt, the first step is to familiarize yourself with the Python language itself.
Today we will quickly review the main notions to get started on it.&lt;/p&gt;

&lt;p&gt;But first, let us install Python!&lt;/p&gt;

&lt;h2 id=&#34;2-python-installation-accompanying-tools&#34;&gt;2. &lt;strong&gt;Python Installation. Accompanying tools&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&#34;2-1-anaconda-python-distribution&#34;&gt;2.1 Anaconda Python Distribution&lt;/h3&gt;

&lt;p&gt;In this course we will use the Python language programming.
However, as mentioned above, one of the main strengths of Python is the large amount of available scientific libraries.
This means that, together with the core Python language, we&amp;rsquo;ll be using several Python packages to perform scientific computations.
But instead of installing all these packages manually one at a time, we will be using a &lt;strong&gt;Python distribution&lt;/strong&gt;.
A distribution consists of the core Python package and several hundred modules, and available through a single download and installation.
In this course, we will use the Anaconda Python distribution.
Apart of Python itself and several hundred libraries, Anaconda also includes two very useful development environments: &lt;strong&gt;Jupyter NB&lt;/strong&gt; and &lt;strong&gt;Spyder&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spyder&lt;/strong&gt; is a Matlab-like &lt;strong&gt;IDE&lt;/strong&gt; (Integrated Development Environment) that we will be using for complex/long programming assignments.
On the other hand, &lt;strong&gt;Jupyter NB&lt;/strong&gt; is a functionality that allows you to run sequentially code in your browser.
I will be giving you instructions in class on how to install and use both tools.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The Anaconda Python distribution can be found &lt;a href=&#34;https://www.continuum.io/downloads&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Please install Python 3.
For windows users, if you are finding troubles, let me know asap.&lt;/p&gt;

&lt;/div&gt;


&lt;h3 id=&#34;2-2-executing-python-code&#34;&gt;2.2  Executing Python code&lt;/h3&gt;

&lt;p&gt;Python has two different modes: &lt;strong&gt;interactive&lt;/strong&gt; and &lt;strong&gt;standard&lt;/strong&gt;.
The interactive mode is meant for experimenting your code one line or one expression at a time.
The standard mode is useful for running programs from start to finish.
You will probably find yourself alternating between both modes.&lt;/p&gt;

&lt;h4 id=&#34;2-2-1-python-interactive-mode&#34;&gt;2.2.1 Python Interactive Mode&lt;/h4&gt;

&lt;p&gt;For a first example on interactive mode, open a command console, type &lt;code&gt;python&lt;/code&gt;, and you will start using Python. Experiment a bit.&lt;/p&gt;

&lt;p&gt;A more convenient way of building code interactively is through &lt;strong&gt;Jupyter iPython notebooks&lt;/strong&gt;.
This also allows us to mix text, code, and maths, which is really cool.
To start an iPython notebook, open a console, type jupyter notebook, and navigate in your browser to &lt;code&gt;http://localhost:8888/&lt;/code&gt;.
In windows, you may need a log-in password, provided automatically after executing jupyter.
&lt;a href=&#34;https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; you can find a series of useful tricks and shortcuts.&lt;/p&gt;

&lt;h4 id=&#34;2-2-2-python-standard-mode&#34;&gt;2.2.2 Python Standard Mode&lt;/h4&gt;

&lt;p&gt;In this case, we write a text file with Python instructions, and run it. &lt;code&gt;hello_world.py&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Again, there is a more convenient way for building code in standard mode, which is using an IDE.
This programming tool will allow you to more easily debug, inspect variables, and quickly modify your code.
They typically also include an embedded interactive system. An example of an IDE is &lt;strong&gt;Spyder&lt;/strong&gt;, already contained in the Anaconda distribution.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;For the contents of this lecture, starting from Section 3, a notebook version of this lecture is available in a static
view &lt;a href=&#34;http://nbviewer.jupyter.org/github/agaldran/daco_2017_practicals/blob/master/lecture_1_python/PL1_introduction2Python.ipynb&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
Please click the download button in the right top corner, download and open it!&lt;/p&gt;

&lt;/div&gt;


&lt;h2 id=&#34;3-introduction-to-the-python-programming-language&#34;&gt;3. &lt;strong&gt;Introduction to the Python Programming Language&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&#34;3-1-fundamental-python-data-types&#34;&gt;3.1 - Fundamental Python data types&lt;/h3&gt;

&lt;p&gt;Python contains several typical built-in data types as part of the core language.
The same as in e.g Matlab (and different from e.g. C) you do not need to explicitly declare the type of a variable.
Python determines data type of variables by how they are used.&lt;/p&gt;

&lt;p&gt;For instance, to create an integer (&lt;code&gt;int&lt;/code&gt;) variable you simply type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# An integer variable a
a = 5
print(type(a))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other basic/common python types are for instance &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;string&lt;/code&gt;, or &lt;code&gt;boolean&lt;/code&gt;, exemplified below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# A float variable f
f = 5.0
# A boolean
b = True
# A string
c = &#39;bom dia&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-2-everything-is-an-object&#34;&gt;3.2 - Everything is an object&lt;/h3&gt;

&lt;p&gt;It is important that you start thinking of the above examples &lt;code&gt;a,f,b,c&lt;/code&gt; as &lt;strong&gt;objects&lt;/strong&gt;.
Each object in Python has three characteristics: object &lt;strong&gt;type&lt;/strong&gt;, object &lt;strong&gt;value&lt;/strong&gt;, and object &lt;strong&gt;identity&lt;/strong&gt;.
Object &lt;strong&gt;type&lt;/strong&gt; tells Python what kind of an object it&amp;rsquo;s dealing with.
A type could be a number, or a string, or a list, or something else.
Object &lt;strong&gt;value&lt;/strong&gt; is the data value is contained by the object.
This could be a specific number, for example.
Finally, you can think of object &lt;strong&gt;identity&lt;/strong&gt; as an identity number for the object.
Each distinct object in the computer&amp;rsquo;s memory will have its own identity number.&lt;/p&gt;

&lt;p&gt;Most Python objects have either data or functions or both associated with them.
These are known as &lt;strong&gt;attributes&lt;/strong&gt;.
The name of the attribute follows the name of the object, and they are separated by a dot in between them.
The two types of attributes are called either &lt;strong&gt;data attributes&lt;/strong&gt; or &lt;strong&gt;methods&lt;/strong&gt;.
A data attribute is a value that is attached to a specific object.
In contrast, a method is a function that is attached to an object.
And typically a method performs some function or some operation on that object.
Object type always determines the kind of operations that it supports.
In other words, depending on the type of the object,
different methods may be available to you as a programmer.
Finally, an &lt;strong&gt;instance&lt;/strong&gt; is one occurrence of an object.
For example, you could have two strings.
They may have different values stored in them,
but they nevertheless support the same set of methods.&lt;/p&gt;

&lt;h3 id=&#34;3-3-python-modules&#34;&gt;3.3 - Python Modules&lt;/h3&gt;

&lt;p&gt;Python contains builtin functions, such as &lt;code&gt;print&lt;/code&gt; that can be used by all Python programs.
However, while the Python library consists of all core elements, such as data types and built-in functions,
the bulk of the Python library consists of &lt;strong&gt;modules&lt;/strong&gt;.
In order for you to be able to make use of modules in your own code,
you first need to import those modules using the &lt;code&gt;import&lt;/code&gt; statement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example of modules, attributes, methods, in numpy.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The numpy example above made available some specific data types and methods for us.
This is an example of a &lt;strong&gt;Python module&lt;/strong&gt;.
In general, Python modules are libraries of code that you import and use through &lt;code&gt;import&lt;/code&gt; statements.
Let&amp;rsquo;s go through a simple example. Import the &lt;code&gt;math&lt;/code&gt; module.
This module gives you access to the pi constant. Print its value.
This module also gives you access to several mathematical operations. Find out (print) the value of the sine of pi.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;If you only need, e.g., the value of pi from the entire &lt;code&gt;math&lt;/code&gt; module, you can import it selectively:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from math import pi
print(pi)
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;h3 id=&#34;3-4-python-basic-operators&#34;&gt;3.4 - Python Basic Operators:&lt;/h3&gt;

&lt;p&gt;Operators are symbols that allow you to use logic and arithmetic in your computations.
Python has several operators, the most prominent ones being &lt;strong&gt;arithmetic&lt;/strong&gt;, &lt;strong&gt;comparison&lt;/strong&gt;, and &lt;strong&gt;logical&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&#34;3-4-1-arithmetic-operators&#34;&gt;3.4.1 - Arithmetic operators:&lt;/h4&gt;

&lt;p&gt;They will take two variables and perform simple mathematical operations on them.
They are addition &lt;code&gt;+&lt;/code&gt;, subtraction &lt;code&gt;-&lt;/code&gt;, multiplication &lt;code&gt;*&lt;/code&gt;, division &lt;code&gt;/&lt;/code&gt;, modulus &lt;code&gt;%&lt;/code&gt;, floor division &lt;code&gt;//&lt;/code&gt;, and exponentiation &lt;code&gt;**&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(3+2, 3-2, 3*2, 3/2, 3%2, 3//2, 3**2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-4-2-comparison-operators&#34;&gt;3.4.2 - Comparison operators:&lt;/h4&gt;

&lt;p&gt;They observe two variables and return a boolean value.
They are the usual greater than (&lt;code&gt;&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;gt;=&lt;/code&gt;), equal (&lt;code&gt;=&lt;/code&gt;), different (&lt;code&gt;!=&lt;/code&gt;), and lower than (&lt;code&gt;&amp;lt;&lt;/code&gt;,&lt;code&gt;&amp;lt;=&lt;/code&gt;) mathematical operators.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(3&amp;gt;2, 3&amp;lt;2, 3==2, 3!=2, 3&amp;gt;=2, 3&amp;lt;=2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-4-3-logical-operators&#34;&gt;3.4.3 - Logical operators:&lt;/h4&gt;

&lt;p&gt;These operators will interpret their input as boolean values, and return a boolean value depending on the truth value of both inputs.
They are &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;, &lt;code&gt;not&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(True and True, False or False, not True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-4-5-other-operators&#34;&gt;3.4.5 - Other operators&lt;/h4&gt;

&lt;p&gt;There are other operators in Python, such as identity (&lt;code&gt;is&lt;/code&gt;, &lt;code&gt;is not&lt;/code&gt;) or membership (&lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not in&lt;/code&gt;).
I am 100% sure you will be able to guess what is their effect on variables/containers!
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Note this subtle difference between &lt;code&gt;==&lt;/code&gt; and &lt;code&gt;is&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;==&lt;/code&gt; tests whether objects have the same value, whereas &lt;code&gt;is&lt;/code&gt;
tests whether objects have the same identity. Try it with &lt;code&gt;a=[1,2]&lt;/code&gt; and &lt;code&gt;b=[1,2]&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/p&gt;

&lt;h3 id=&#34;3-5-sequences-and-object-containers&#34;&gt;3.5 - Sequences and Object Containers&lt;/h3&gt;

&lt;h4 id=&#34;3-5-1-python-sequences&#34;&gt;3.5.1 - Python Sequences&lt;/h4&gt;

&lt;p&gt;A sequence is an &lt;strong&gt;ordered&lt;/strong&gt; collection of objects. In Python, you can find three types of sequences: Lists, Tuples, and Range Objects.&lt;/p&gt;

&lt;p&gt;Let us focus on the first two of them. Lists and tuples can be accessed by indexing and can be sliced in several ways:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# A tuple
t = (0,True)
# A list l
l = [0,True]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that there is a relevant difference between tuples and lists: tuples are &lt;strong&gt;immutable&lt;/strong&gt;, while lists are not.
This means that you won&amp;rsquo;t be able to modify the content stored at &lt;code&gt;t&lt;/code&gt;. We will explain this in a second.
Note also that both lists and tuples allow you to mix different data types.&lt;/p&gt;

&lt;h4 id=&#34;note-mutable-and-immutable-objects&#34;&gt;Note: Mutable and immutable objects&lt;/h4&gt;

&lt;p&gt;The value of some objects can change in the course of program execution.
Objects whose value can change are said to be mutable objects,
whereas objects whose value is unchangeable after they&amp;rsquo;ve been created
are called immutable.&lt;/p&gt;

&lt;p&gt;Continuing with sequences, in order to access the elements that a tuple/list holds, you use &lt;strong&gt;square brackets, not parenthesis&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# the first element of the tuple t
t_first = t[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also note that, the same as e.g. C (and different from e.g. Matlab), &lt;strong&gt;in Python indexing starts at 0&lt;/strong&gt;.
Be careful with this, because it is a common source of confusion in the beginning.&lt;/p&gt;

&lt;p&gt;Sequences can be accessed by indexing, which supports negatives indexes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t = (0,1,&#39;hola&#39;,3,4.0)
l = [0,1,&#39;hola&#39;,3,4.0]
print(t[0])
print(t[-1]) # note the behavior of negative indexes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sequences also support a very handy operation called &lt;strong&gt;slicing&lt;/strong&gt;, that enables access to multiple objects at the same time:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(t[0:2]) # slicing first two elements, third is excluded
print(l[2:5]) # slicing last three elements
print(l[2:]) # empty spot after : means up to length-of-list index
print(l[:2]) # empty spot before : means from first index
print(l[0:5:2]) # every element, but use a step size of 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tuples and lists, as every python object, have several methods that you can use with them.
However, since lists are mutable, they have methods that can modify their content:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = [4,3,2,1]
a.append(5)
print(a)
a.sort()
print(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that list methods are &lt;strong&gt;in-place methods&lt;/strong&gt;, they modify the original object that called them and return nothing.
For a complete list of these, see &lt;a href=&#34;http://faculty.salina.k-state.edu/tim/NPstudy_guide/python/containers.html#list-methods&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are other generic Python functions that work with sequences and provide useful operations:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = [4,3,2,1]
b = sorted(a)
n = len(b)
s = sum(a)
print(b, n, s)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-5-2-object-containers-dictionaries&#34;&gt;3.5.2 - Object Containers: Dictionaries&lt;/h4&gt;

&lt;p&gt;Sequences are a particularly simple example of Object Containers.
More generally, Python offers several more advanced object containers. A really useful one are dictionaries.&lt;/p&gt;

&lt;p&gt;Dictionaries are &lt;strong&gt;unordered sequences&lt;/strong&gt; that associate key objects to value objects.
This means that a dictionary consists of &lt;strong&gt;Key:Value&lt;/strong&gt; pairs, and the keys must be immutable while the values can be anything.
Note that dictionaries themselves are mutable objects.&lt;/p&gt;

&lt;p&gt;A dictionary is built with curly braces as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ages = {&amp;quot;Me&amp;quot;: 33, &amp;quot;You&amp;quot;: 22, &amp;quot;Him&amp;quot;:24}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If now I want to know your age, and then increase it, I would type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(ages[&amp;quot;You&amp;quot;])
ages[&amp;quot;You&amp;quot;] += 1
print(ages[&amp;quot;You&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can add new items to your dictionary as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ages[&amp;quot;Her&amp;quot;] = 20
print(ages)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dictionary objects have also their own methods.
For instance, you can use &lt;code&gt;keys&lt;/code&gt; to find out what are all the keys in the dictionary, or the &lt;code&gt;values&lt;/code&gt; method
to retrieve are all of the values in the dictionary:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;names = ages.keys()
years = ages.values()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-6-python-typing&#34;&gt;3.6 - Python typing&lt;/h3&gt;

&lt;p&gt;Objects in Python are composed by their name, their content, and a reference that points the name to the content.
When an object is created, Python tells the computer to reserve memory locations to store the corresponding data.
Depending on the data type of a variable, the interpreter allocates a certain amount of memory.&lt;/p&gt;

&lt;p&gt;As we have seen above, in Python, there is no need of declaring objects nor their type before using them.
This is because actually what you are doing is not creating a spot in memory and filling it with an object.
Rather, you are creating a pointer (that occupies a first memory spot), and then making that pointer point to an object in a second memory spot.
For this reason, you can for instance reassign a variable to a different type of object without errors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = [0,1,2]
a = (-1,3)
a = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that when you assign a variable to another, you are just creating a second pointer to that same memory spot.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = [0,1,2]
b = a
b[2] = 0
print(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-7-flow-control&#34;&gt;3.7 Flow Control&lt;/h3&gt;

&lt;p&gt;Typical flow control structures are implemented as usual in Python.
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Be careful with Python code &lt;strong&gt;indentation&lt;/strong&gt;:
the space you leave to the left of your piece of code implicitly delimits code blocks.&lt;/p&gt;

&lt;/div&gt;

We will learn the behavior of flow control statements in Python by example,
to reinforce the idea of how intuitive Python is.&lt;/p&gt;

&lt;h4 id=&#34;3-7-1-if-else-statements&#34;&gt;3.7.1 - &lt;code&gt;if&lt;/code&gt;-&lt;code&gt;else&lt;/code&gt; statements&lt;/h4&gt;

&lt;p&gt;Observe the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if 3&amp;gt;2:
    print(&#39;success&#39;)
elif 3==2:
    print(&#39;failure&#39;)
else:
    print(&#39;I do not know&#39;)
print(&#39;This will be printed either way&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Do not forget about the semicolon in the end of control statements!&lt;/p&gt;

&lt;/div&gt;


&lt;h4 id=&#34;3-7-2-for-loops&#34;&gt;3.7.2 - &lt;code&gt;for&lt;/code&gt; loops&lt;/h4&gt;

&lt;p&gt;Observe the following code and try to predict its output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in [0,1,2,3]:
    print(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-7-3-while-loops&#34;&gt;3.7.3 - &lt;code&gt;while&lt;/code&gt; loops&lt;/h4&gt;

&lt;p&gt;Observe the following code and try to predict its output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i=0
while i &amp;lt; 4:
    print(i)
    i = i+1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-7-4-other-statements-break-and-continue&#34;&gt;3.7.4 - Other statements: &lt;code&gt;break&lt;/code&gt; and &lt;code&gt;continue&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Observe the following two pieces of code and try to predict their output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in [1,2,3,4,5,6,7]:
    if i % 3 == 0:
        continue
    print(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in [1,2,3,4,5,6,7]:
    if i % 3 == 0:
        break
    print(i)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Can you give a definition of both statements based on these experiments?&lt;/p&gt;

&lt;/div&gt;


&lt;h3 id=&#34;3-8-python-functions&#34;&gt;3.8 Python Functions&lt;/h3&gt;

&lt;p&gt;Being only able to ``interactively&amp;rdquo; play with variables is boring.
To build more complex code, we need functions.
Functions are tools for grouping statements so that they can be executed more than once in the same program.
They are useful maximize code reuse and minimize code redundancy, therefore contributing to avoid errors.&lt;/p&gt;

&lt;p&gt;Functions are written using the &lt;code&gt;def&lt;/code&gt; statement.
You can send objects created inside your function back to where it was called with the &lt;code&gt;return&lt;/code&gt; statement.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_sum(a,b):
    c = a+b
    return c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use this function, we simply call it passing appropriate parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = 5
b = -2
print(compute_sum(a,b))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Arguments to Python functions are matched by position.
Tuples are typically used to return multiple values.
Note that functions themselves are objects:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(type(compute_sum))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In general, variables created or assigned in a function are local of that function and exist only while the function runs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;L = [0,1,2]
def modify(my_list):
    c = 3
    my_list[0] += 20
modify(L)
print(L)
print(c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is also possible to specify a &lt;strong&gt;default&lt;/strong&gt; value for some argument:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def compute_sum(a,b=2):
    c = a+b
    return c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Likewise, you can have keyword arguments.
A keyword argument is an argument which is supplied to the function by explicitly naming each parameter and specifying its value:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(1, 2, 3, 4, sep=&#39;;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Keyword arguments must always go behind non-keyword arguments.&lt;/p&gt;

&lt;h3 id=&#34;3-9-classes-and-object-oriented-programming&#34;&gt;3.9 Classes and Object-Oriented Programming&lt;/h3&gt;

&lt;p&gt;How can you go beyond built-in data types and create &lt;strong&gt;new object types&lt;/strong&gt;, with their
associated methods and attributes defined by you?&lt;/p&gt;

&lt;p&gt;Python allows you to create new classes, and then define (&lt;em&gt;instantiate&lt;/em&gt;)
new objects of that class and interact with them. This way, you can
group data and functions operating on it in a more abstract way, and
then instantiate concrete samples and use them.
Classes allow for a simplified modeling of our problems, and enables the
creation of cleaner code that will be more easily extended in the future.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;When dealing with classes, data is usually called &lt;em&gt;attributes&lt;/em&gt;, and functions &lt;em&gt;methods&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;


&lt;h4 id=&#34;3-9-2-building-a-new-class-from-scratch&#34;&gt;3.9.2 - Building a new Class from scratch&lt;/h4&gt;

&lt;p&gt;Every class needs to have a special method, called &lt;code&gt;constructor&lt;/code&gt;, that initializes its attributes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class UP_student:
    def __init__(self, name, math_skills, coding_skills, hard_working, theory_mark, practical_mark):
        self.name = name
        self.math_skills = math_skills
        self.coding_skills = coding_skills
        self.hard_working = hard_working                
        self.theory_mark = theory_mark
        self.practical_mark = practical_mark  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will note the presence of the &lt;code&gt;self&lt;/code&gt; parameter: this is a special inner
reference to the object state. It may take some time to understand the use
of &lt;code&gt;self&lt;/code&gt;, but do not be afraid, we will see some examples afterwards.&lt;/p&gt;

&lt;p&gt;As it stands, an object of the &lt;code&gt;UP_student&lt;/code&gt; class has very limited value, as it contains only data (attributes).
Let us add some spice by giving our class a function (method):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Y:
    def __init__(self, v0):
        self.v0 = v0
        self.g = 9.81
    def value(self, t):
        return self.v0 * t - 0.5*self.g*t**2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The utility of &lt;code&gt;self&lt;/code&gt; starts to become clear now. At this point, you have
created a useful class, and you can instantiate an object of this new type easily:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;name = &#39;adrian_galdran&#39;
adrian_math_skills = 0.9
adrian_coding_skills = 0.8
adrian_hard_working = True
adrian_student = UP_student(name, adrian_math_skills, adrian_coding_skills, adrian_hard_working)
print(type(a_student))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, we call our class as if it was a normal Python function, and Python automatically invokes the constructor method.
&lt;code&gt;__init__&lt;/code&gt; requires several parameters to be specified at instantiation time. If you do not specify them correctly, you will get an error.&lt;/p&gt;

&lt;p&gt;Now, attributes and methods are exposed to the user:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(adrian_student.coding_skills)
print(adrian_student.hard_working)
print(&#39;Global Mark: &#39;, adrian_student.compute_global_mark(0.2)) # Let us give more weight to the practical part!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How can you add new methods to your class? For instance, we can add a &lt;code&gt;print_global_mark&lt;/code&gt; method
that computes and prints the final mark automatically. This method only needs as input parameter &lt;code&gt;theory_weight&lt;/code&gt;, and outputs a string:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class UP_student:
    def __init__(self, name, math_skills, coding_skills, hard_working, theory_mark = 5, practical_mark = 4):
        self.name = name
        self.math_skills = math_skills
        self.coding_skills = coding_skills
        self.hard_working = hard_working                
        self.theory_mark = theory_mark
        self.practical_mark = practical_mark 
    def compute_global_mark(self, theory_weight = 0.6):
        return theory_weight*self.theory_mark + (1-theory_weight)*self.practical_mark
    def print_global_mark(self, theory_weight = 0.6):
        global_mark = self.compute_global_mark(theory_weight)
        print(&#39;The final mark of &#39;, self.name, &#39; is &#39;, global_mark)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that even if the &lt;code&gt;print_global_mark&lt;/code&gt; method only needs the &lt;code&gt;theory_weight&lt;/code&gt; argument, we still
must add the &lt;code&gt;self&lt;/code&gt; argument so that it can call &lt;code&gt;self.global_mark&lt;/code&gt;.
This is omitted in the method call.&lt;/p&gt;

&lt;p&gt;Notice also that inside the class, &lt;code&gt;compute_global_mark&lt;/code&gt; is known to the object and needs no &lt;code&gt;self&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;name = &#39;adrian_galdran&#39;
adrian_math_skills = 0.9
adrian_coding_skills = 0.8
adrian_hard_working = True
adrian_student = UP_student(name, adrian_math_skills, adrian_coding_skills, adrian_hard_working)

adrian_student.print_global_mark()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We know an object consists of both internal data and methods that perform operations on the data.
At some point you may find that existing object types do not fully suit your needs.
Classes are the tool that allows you to create new types of objects.&lt;/p&gt;

&lt;h4 id=&#34;3-9-2-class-inheritance&#34;&gt;3.9.2 - Class Inheritance&lt;/h4&gt;

&lt;p&gt;Sometimes, even if you have the need for a new type, it may happen that this new object type resembles,
in some way, an existing one.
Classes have the ability to inherit from other classes, and this is a fundamental aspect of OOP.&lt;/p&gt;

&lt;p&gt;Let us see an example of how to build a new class, inheriting from the built-in Python &lt;code&gt;list&lt;/code&gt; class.
We will add more functionality to it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MyList(list):
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;this definition ensure that our new class, derived from &lt;code&gt;list&lt;/code&gt;, will inherit the attributes of the base class.
However, now we can extend, or redefine those attributes!&lt;/p&gt;

&lt;p&gt;For instance, we are going to improve the built-in &lt;code&gt;remove&lt;/code&gt; methods, implemented by Python for lists in this way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;L = [0,1,2,5,5]
L.remove(5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will add new methods to also be able to remove the maximum and minimum element of a list.
For this, we complete the definition of our extended class as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MyList(list):
    def remove_min(self):
        self.remove(min(self))
    def remove_max(self):
        self.remove(max(self))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can make use of our class:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;L2 = MyList(L)
dir(L)
dir(L2)
print(L2.remove_min())
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-complementary-python-scientific-computing-tools-numpy&#34;&gt;4. &lt;strong&gt;Complementary Python Scientific Computing Tools: Numpy&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;As mentioned in the introduction, one of the most important strengths of Python is the large ecosystem of tools available.
One of the most important libraries for scientific computing in general (and for this course in particular) is &lt;strong&gt;NumPy&lt;/strong&gt;,
which is designed to perform matrix computations.
Here you will learn the fundamental concepts related to Numpy.&lt;/p&gt;

&lt;h3 id=&#34;4-1-introduction-to-numpy-arrays&#34;&gt;4.1 - Introduction to NumPy Arrays&lt;/h3&gt;

&lt;p&gt;Python allows you to create nested lists, that you could use to work with n-dimensional arrays:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;zero_matrix = [[0,0,0],[0,0,0]]
print(len(zero_matrix),len(zero_matrix[0]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, you can see this is quite inconvenient, and complexity will grow a lot with high dimensions.
Also, Python lists are not designed for linear algebra. For instance, &lt;code&gt;+&lt;/code&gt; acting on lists means concatenation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print([1,2]+[0,0])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NumPy arrays are n-dimensional array objects which conform the core component of scientific computing in Python.
They are an additional data type provided by NumPy for representing vectors and matrices.
Unlike Python lists, elements of NumPy arrays are all of the same data type, and their size is fixed once defined.
By default, the elements are floating point numbers.&lt;/p&gt;

&lt;p&gt;This is a first example of how to build a vector and a matrix with all elements zero.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

zero_vector = np.zeros(4)
zero_matrix = np.zeros((2,3))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that in the matrix case, we need to specify the dimensions through a tuple.
In order to build an array of ones, you can use the &lt;code&gt;numpy.ones&lt;/code&gt; function, with the same syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ones_matrix = np.ones((3,2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, you can manually initialize them using &lt;code&gt;np.array&lt;/code&gt; and a Python list:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_matrix = np.array([[2,1],[3,2],[5,4]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;numpy&lt;/code&gt; supports the usual standard matrix operations, such as matrix transposition;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_transposed_matrix = my_matrix.transpose()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Arithmetic operations work as expected also:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_matrix + ones_matrix
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that unlike MATLAB, &lt;code&gt;*&lt;/code&gt; is elementwise multiplication, not matrix multiplication.
You need to use the &lt;code&gt;dot&lt;/code&gt; function to compute products of vectors, to multiply a vector by a matrix, and to multiply matrices.
&lt;code&gt;dot&lt;/code&gt; is available both as a function in the numpy module and as an instance method of array objects:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.array([[1,2],[3,4]]) # 2x2 Matrix
y = np.array([[5,6],[7,8]]) # 2x2 Matrix

v = np.array([9,10]) # 1x2 vector
w = np.array([11, 12]) # 1x2 vector

# Matrix / vector product; both produce a 1x2 vector
print(x.dot(v))
print(np.dot(x, v))

# Matrix / matrix product; both produce a 2x2 matrix
# [[19 22]
#  [43 50]]
print(x.dot(y))
print(np.dot(x, y))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, you can find out the dimensions of a given &lt;code&gt;numpy&lt;/code&gt; array through its &lt;code&gt;shape&lt;/code&gt; data attribute:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(my_matrix.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-2-slicing-numpy-arrays&#34;&gt;4.2 - Slicing NumPy Arrays&lt;/h3&gt;

&lt;p&gt;The same way you can slice Python lists, you can do the same with &lt;code&gt;numpy&lt;/code&gt; arrays.
Remember the indexing logic.
Start index is included but stop index is not, so &lt;code&gt;Python&lt;/code&gt; stops just before it reaches the stop index.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_first_column = my_matrix[:,0]
my_last_row = my_matrix[-1,:]
print(my_first_column.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-3-indexing-numpy-arrays&#34;&gt;4.3 - Indexing NumPy Arrays&lt;/h3&gt;

&lt;p&gt;NumPy arrays can also be indexed with other arrays or other sequence-like objects like lists. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z1 = np.array([2,4,6,8,10])
z2 = z1+1
indexes_arr = np.array([0,1])
indexes_list = [0, 4]
print(z2, z2[indexes_arr], z2[indexes_list])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another way of indexing &lt;code&gt;numpy&lt;/code&gt; arrays is with logical indices:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;indexes_logical = [True, True, True, False, False]
print(z2[indexes_logical])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the potential of this operation!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(z2&amp;gt;5)
print(z2[z2&amp;gt;5])
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;important-difference-between-slicing-and-indexing&#34;&gt;Important difference between slicing and indexing&lt;/h4&gt;

&lt;p&gt;When you slice an array with the colon operator, you obtain a &lt;strong&gt;view&lt;/strong&gt; of the object.
This means that if you modify that view, you will also modify the original array.
In contrast, when you index an array, what you obtain is a (new) object, a copy independent of the original one.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z1 = np.array([2,4,6,8,10])
w_view = z1[0:3] # sliced z1
print(z1)
print(w_view)
w_view[0] = 50
print(z1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare the above code snippet with this one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;z1 = np.array([2,4,6,8,10])
indexes = [0,1,2,3,4]
w_copy = z1[indexes] # indexed z1
print(z1)
print(w_copy)
w_copy[0] = 50
print(z1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-4-other-numpy-arrays-manipulation-techniques&#34;&gt;4.4 - Other numpy arrays manipulation techniques&lt;/h3&gt;

&lt;p&gt;In &lt;code&gt;numpy&lt;/code&gt;, if you want to build an array with fixed start and end values, such that the other elements are uniformly
spaced between them, you can do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.linspace(0, 100, 10) # stop point is included
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have already seen an example of the attribute &lt;code&gt;shape&lt;/code&gt; of a &lt;code&gt;numpy&lt;/code&gt; array. You can also check the total &lt;code&gt;size&lt;/code&gt; of it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_matrix = np.array([[2,1],[3,2],[5,4]])
print(my_matrix.shape)
print(my_matrix.size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that neither &lt;code&gt;shape&lt;/code&gt; nor &lt;code&gt;size&lt;/code&gt; are followed by a parenthesis.
This is because they are not method attributes for the &lt;code&gt;numpy&lt;/code&gt; array class, but rather data attributes.&lt;/p&gt;

&lt;p&gt;There are a couple of handy logical operations that work on top of &lt;code&gt;numpy&lt;/code&gt; arrays.
For instance, you often will want to check if any/all of the elements in an array verifies a given condition.
This can be accomplished with the methods &lt;code&gt;any()&lt;/code&gt; and &lt;code&gt;all()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.random.random(10)
print(x)
print(np.any(x&amp;gt;0.5))
print(np.all(x&amp;gt;0.5))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, instead of using the Python &lt;code&gt;random&lt;/code&gt; library, we used the &lt;code&gt;numpy.random&lt;/code&gt; module.&lt;/p&gt;

&lt;h2 id=&#34;5-complementary-python-scientific-computing-tools-matplotlib&#34;&gt;5. &lt;strong&gt;Complementary Python Scientific Computing Tools: Matplotlib&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Matplotlib is the standard Python plotting library.
Even if &lt;code&gt;matplotlib&lt;/code&gt; is a very large library, it contains a module called &lt;code&gt;pyplot&lt;/code&gt;.
Pyplot is a collection of functionalities that make matplotlib work in a similar way as Matlab.
In this course, we will use &lt;code&gt;pyplot&lt;/code&gt; for our data visualizations.&lt;/p&gt;

&lt;h3 id=&#34;5-1-basic-matplotlib-usage&#34;&gt;5.1 - &lt;strong&gt;Basic Matplotlib usage&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Let us import it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The most basic function inside &lt;code&gt;pyplot&lt;/code&gt; is &lt;code&gt;plot&lt;/code&gt;.
Its simplest use case takes only one argument, specifying  the y-axis values that are to be plotted.
In this case, each y-axis value is plotted against its corresponding index value on the x-axis:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = np.random.random(10)
plt.plot(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you use &lt;code&gt;plot&lt;/code&gt; outside the iPython shell, the plot is created but not shown.
To tell Python to show the plot, you just need to add &lt;code&gt;plt.show()&lt;/code&gt; to your code.&lt;/p&gt;

&lt;p&gt;When you give to &lt;code&gt;plot&lt;/code&gt; two arguments, the first argument specifies the x-coordinates and the second the y-coordinates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(0,10,100)
y = np.cos(x)
plt.plot(x,y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also supply a third argument to &lt;code&gt;plot&lt;/code&gt; in order to give some cosmetic specifications on your plot, like color, line type or marker.
They work with key-word arguments.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(0,10,20)
y = np.cos(x)
plt.plot(x, y, &#39;ro-&#39;)
plt.show()
plt.plot(x, y, &#39;gs-&#39;, linewidth=5, markersize=15)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that in this case &lt;code&gt;plt.show()&lt;/code&gt; forces Python to show the first plot, which otherwise would be ommited.&lt;/p&gt;

&lt;h3 id=&#34;5-2-plot-customization&#34;&gt;5.2 - &lt;strong&gt;Plot Customization&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Let us see some more advanced plot customization techniques.
To add a legend to an already created (even if still not shown) plot, you can use &lt;code&gt;legend()&lt;/code&gt;, which takes a string as an argument:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(-3,3,20)
y = x**2
plt.plot(x, y, &#39;ro-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to add information on which quantities are specified on each axis, you can do it as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(-3,3,20)
y = x**2
plt.plot(x, y, &#39;ro-&#39;)
plt.xlabel(&#39;The x axis&#39;)
plt.ylabel(&#39;The y axis&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also customize what part of your plot you want to display with &lt;code&gt;axis()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(-3,3,20)
y = x**2
plt.plot(x, y, &#39;ro-&#39;)
plt.axis([-0.5, 2, -2, 4]) #xmin, xmax, ymin, ymax
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is also quite easy to overlay several plots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(-3,3,20)
y1 = x**2
y2 = x**3
plt.plot(x, y1, &#39;ro-&#39;)
plt.plot(x,y2, &#39;b+-&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you need to add an independent legend to each of these, you need to label each of them separately:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(-3,3,20)
y1 = x**2
y2 = x**3
plt.plot(x, y1, &#39;ro-&#39;, label = &#39;square&#39;)
plt.plot(x,y2, &#39;b+-&#39;, label = &#39;cubic&#39;)
plt.legend(loc = &#39;upper left&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that &lt;code&gt;legend()&lt;/code&gt; can take a keyword argument specifying location.&lt;/p&gt;

&lt;p&gt;Finally, to save your figure, you simple use &lt;code&gt;savefig&lt;/code&gt;.
The extension of the file name you choose will determine the format of the output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.linspace(-3,3,20)
y1 = x**2
plt.plot(x, y1, &#39;ro-&#39;, label = &#39;square&#39;)
plt.savefig(&#39;my_plot.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, let us illustrate the use of histogram plotting tools in matplotlib, as well as how to build several subplots in the same plot.
First, we create a normally distributed array of numbers around zero using numpy as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.random.normal(size = 1000)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To build a histogram plot in pyplot we type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.hist(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, if you want to plot the same histogram with two different colors in two different subplots, you can use the &lt;code&gt;plt.subplot()&lt;/code&gt; function.
This function takes three arguments: the first two specify the number of rows and columns in the subplot, and the third one is the plot number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.subplot(1,2,1)
plt.hist(x, color = &#39;r&#39;);
plt.subplot(1,2,2)
plt.hist(x, color = &#39;b&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the use of a semicolon after each plot execution, in order to avoid printing the value returned by &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;6-homework&#34;&gt;6. &lt;strong&gt;Homework&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;For now, you can access a notebook with an exercise on Python classes &lt;a href=&#34;http://nbviewer.jupyter.org/github/agaldran/daco_2017_practicals/blob/master/lecture_1_python/DACO17_hw_exercise_1.ipynb&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I will be adding another problem in the next days.&lt;/p&gt;

&lt;h2 id=&#34;7-sources-and-references&#34;&gt;7. &lt;strong&gt;Sources and References&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Of course, there are tons of wonderful Python resources in the internet.
The main sources I used to build this lecture were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Jake Van der Plas&amp;rsquo; book &lt;em&gt;A Whilrwind Tour of Python&lt;/em&gt;, available &lt;a href=&#34;http://www.oreilly.com/programming/free/files/a-whirlwind-tour-of-python.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;
in pdf, and &lt;a href=&#34;https://github.com/jakevdp/WhirlwindTourOfPython&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; in iPython notebook format.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Harvard&amp;rsquo;s online course &lt;em&gt;Using Python for Research&lt;/em&gt;, hosted at &lt;a href=&#34;https://courses.edx.org/courses/course-v1:HarvardX+PH526x+3T2016/course/&#34; target=&#34;_blank&#34;&gt;edX&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stanford&amp;rsquo;s CS231n short Python tutorial &lt;a href=&#34;http://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In general, most of the material and presentation is shamelessly inspired in 2.&lt;/p&gt;

&lt;p&gt;Regarding the &lt;code&gt;numpy&lt;/code&gt; part, if you are a Matlab user, you could find this resource very useful:
- Numpy for Matlab users - &lt;a href=&#34;http://scipy.github.io/old-wiki/pages/NumPy_for_Matlab_Users&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you need or want more practice with Python and the tools presented today, I would recommend following the free course
at &lt;a href=&#34;https://www.datacamp.com/courses/intro-to-python-for-data-science&#34; target=&#34;_blank&#34;&gt;datacamp&lt;/a&gt;, and doing all the exercises proposed there.
Codeacademy exercises, hosted &lt;a href=&#34;https://www.codecademy.com/learn/learn-python&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, can also be very useful to get more experience.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DACO - Practical Lecture 2 - Bayesian Classification and Linear Regression</title>
      <link>https://agaldran.github.io/post/17_daco_prac_lec_2/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0100</pubDate>
      
      <guid>https://agaldran.github.io/post/17_daco_prac_lec_2/</guid>
      <description>

&lt;p&gt;This practical lecture is again based on Jupyter Notebooks.
You can see and download the notebook corresponding to today &lt;a href=&#34;https://github.com/agaldran/daco_2017_practicals/blob/master/lecture_2_bayes_regression/PL2_bayesian_classification_LR_new.ipynb&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
For completeness, please also download the following set of Python files, which will illustrate the use of Jupyter notebooks in combination with Spyder:
&lt;a href=&#34;https://github.com/agaldran/daco_2017_practicals/blob/master/lecture_2_bayes_regression/main.py&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;,
&lt;a href=&#34;https://github.com/agaldran/daco_2017_practicals/blob/master/lecture_2_bayes_regression/daco_create_data.py&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;,
&lt;a href=&#34;https://github.com/agaldran/daco_2017_practicals/blob/master/lecture_2_bayes_regression/daco_visualizations.py&#34; target=&#34;_blank&#34;&gt;[3]&lt;/a&gt;.
Please store them in the same folder as where you put the notebook.&lt;/p&gt;

&lt;p&gt;Note that we discuss Linear Regression in Theoretical Lecture 6.
For this reason, we will only be using them alsmot as a black-box today, to familiarize ourselves with sk-learn for regression tasks.&lt;/p&gt;

&lt;p&gt;Here is the plan for today:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Motivation and Goals. Machine Learning for Supervised Classification and Regression&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Difference between Classification and Regression&lt;/li&gt;
&lt;li&gt;Difference between Generative and Discriminative models&lt;/li&gt;
&lt;li&gt;Aside: Working from Spyder&lt;/li&gt;
&lt;li&gt;Quick overview of scikit-learn&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bayesian Classification: a Bit of Theory&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Naive Bayes Classifier Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;From scratch&lt;/li&gt;
&lt;li&gt;Using scikit-learn&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linear Regression: a (tiny) Bit of Theory&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linear Regression Using scikit-learn&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Homework ðŸ˜±&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sources and References&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;1-motivation-and-goals-machine-learning-for-supervised-classification-and-regression&#34;&gt;1.- &lt;strong&gt;Motivation and Goals. Machine Learning for Supervised Classification and Regression&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;As you will know, this course is all about about making predictions out of data and annotations.
But there are several different kinds of predictions, and hence several tasks that we can solve.
Among the most fundamental ones are Supervised Classification and Supervised Regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The term &lt;strong&gt;supervised&lt;/strong&gt; refers to the fact that we assume we have data with annotations.
A different kind of models exist related to the situation where there is no annotations, and the goal is to extract some useful information out of the available (unlabeled) data. This is called &lt;strong&gt;unsupervised&lt;/strong&gt; learning.&lt;/p&gt;

&lt;p&gt;For instance, we could think of a dataset with customer preferences regarding certain products.
Could we group a given pool of users into several subgroups that behave similar among them, but differently with respect to the other subgroups?
This task is known as clustering, and we will be working on it in future lectures.
But for now, we assume for each example in our dataset, we know the corresponding quantity of interest.&lt;/p&gt;

&lt;h2 id=&#34;1-1-difference-between-classification-and-regression&#34;&gt;1.1 - &lt;strong&gt;Difference between Classification and Regression&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: in your data every examples is associated to two or more classes. You want to learn from this labeled data how to predict the class of new unlabeled data.&lt;/p&gt;

&lt;p&gt;Example: Given a set of lung CT scans, a doctor has annotated each of them after examination, specifying whether they show or not signs of lung cancer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt;: In this case, the output of your model is is not a discrete label, but a real number.&lt;/p&gt;

&lt;p&gt;Example: given a training set in which you have examples of red and white globule concentration, predict the amount of blood pressure. In this case, the output (blood pressure) is not a single label, but a number.&lt;/p&gt;

&lt;p&gt;Both classification and regressoin are supervised learning problems, since there is labeled data available. However, the discrete nature of classification, as opposed to continuous for regression, require different types of learning models.&lt;/p&gt;

&lt;h2 id=&#34;1-2-difference-between-generative-and-discriminative-models&#34;&gt;1.2 - &lt;strong&gt;Difference between Generative and Discriminative models&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In (supervised) learning we always have a dataset of examples with associated labels.
However, we do not usually feed our models with the raw data (with a super-relevant exception that we will see later in this course).&lt;/p&gt;

&lt;p&gt;For instance, imagine I have a dataset of all the movies I watched in 2016 and 2017, and each movie is tagged with a label indicating whether I liked it or not.
If I want to build a model that takes a new movie and predicts if I will like it or not, I am not going to give to my classifier every frame of the movie and let it process it. Rather I am going to extract some key information summarizing the movie, such as genre, actors and actresses, duration, soundtrack, etc.
We call these key informative traits of our data &lt;strong&gt;features&lt;/strong&gt;, and their selection is&lt;/p&gt;

&lt;p&gt;Imagine now that I decided that with the duration and rating of the movie in IMDB, I can already describe the movie in such a way that I can build a classifier for my problem.
We can build the following plot:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
%matplotlib inline

# create some synthetic data
n_samples = 100
n_features = 2
movies_mean = [[1.5, 7], [2, 8]] # [mean_movie_duration, mean_IMDB_score]
movies_std = (0.25, 0.25) # [std_movie_duration, std_IMDB_score]
movies, like_it_or_not = make_blobs(n_samples, n_features, movies_mean, movies_std)


movies_I_liked = movies[like_it_or_not==1]
movies_I_disliked = movies[like_it_or_not==0]

# plot data points
plt.scatter(movies_I_liked[:,0],movies_I_liked[:,1], marker = &#39;o&#39;, color = &#39;g&#39;, label = &#39;Liked&#39;);
plt.scatter(movies_I_disliked[:,0],movies_I_disliked[:,1], marker = &#39;o&#39;, color = &#39;r&#39;, label = &#39;Disliked&#39;);
plt.xlabel(&#39;duration&#39;);
plt.ylabel(&#39;IMDB rating&#39;);
plt.legend();
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig1.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Roughly speaking, what a discriminative model will do in this case is trying to find something called &lt;em&gt;decision boundary&lt;/em&gt; that allows us to tell apart movies I liked from movies I did not like.&lt;/p&gt;

&lt;p&gt;For instance, it could be a simple linear decision boundary, a predictive model stating that if the movie has a duration of less than 1,75 hours, I will like it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = 1.75*np.ones(100)
y = np.linspace(6.0,8.7,100)
plt.scatter(movies_I_liked[:,0],movies_I_liked[:,1], marker = &#39;o&#39;, color = &#39;g&#39;, label = &#39;Liked&#39;);
plt.scatter(movies_I_disliked[:,0],movies_I_disliked[:,1], marker = &#39;o&#39;, color = &#39;r&#39;, label = &#39;Disliked&#39;);
plt.xlabel(&#39;duration&#39;);
plt.ylabel(&#39;IMDB rating&#39;);
plt.legend();
plt.plot(x,y, lw = 2)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig2.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast, a generative model would try to find a model that &lt;strong&gt;explains&lt;/strong&gt; the way the data was generated for both classes (liked/disliked). After these models have been built, given a new example, we can inspect which of the two models was more likely to have generated that example.&lt;/p&gt;

&lt;p&gt;For instance, we could model both distributions (liked/disliked) as being generated by a Gaussian distribution. Since I created the above synthetic data, I already know that it comes from two different distributions, parametrized by $(\mu_1, \sigma_1) = (1.5, 7)$ and $(\mu_2, \sigma_2) = (2, 8),$ and I can for instance draw a nice contour plot:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import multivariate_normal

rv = multivariate_normal([1.5, 7], [[0.25, 0], [0, 0.25]]) # normal distribution around (1.5, 7)
rv_2 = multivariate_normal([2, 8], [[0.25, 0], [0, 0.25]]) # normal distribution around (2, 8)

x_grid, y_grid = np.mgrid[0.25 : 3.25: 0.1, 6 : 9: 0.1]
pos = np.empty(x_grid.shape + (2,))
pos[:, :, 0] = x_grid; pos[:, :, 1] = y_grid

plt.contour(x_grid, y_grid, rv.pdf(pos), linewidths=0.5, colors = &#39;r&#39;)
plt.contour(x_grid, y_grid,rv_2.pdf(pos), linewidths=0.5, colors = &#39;g&#39;)

plt.scatter(movies_I_liked[:,0],movies_I_liked[:,1], marker = &#39;o&#39;, color = &#39;g&#39;, label = &#39;Liked&#39;);
plt.scatter(movies_I_disliked[:,0],movies_I_disliked[:,1], marker = &#39;o&#39;, color = &#39;r&#39;, label = &#39;Disliked&#39;);
plt.xlabel(&#39;duration&#39;);
plt.ylabel(&#39;IMDB rating&#39;);
plt.legend(loc = &#39;upper left&#39;);
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig3.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The fact that we know the model that the data distribution follows has several advantages. We can for instance sample, i.e. generate, new data points for each of the two classes. This is the origin of the term &lt;em&gt;generative&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Quoting &lt;a href=&#34;https://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html&#34; target=&#34;_blank&#34;&gt;David Duvenaud&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Generative modeling loosely refers to building a model of data, for instance p(image), that we can sample from. This is in contrast to discriminative modeling, such as regression or classification, which tries to estimate conditional distributions such as p(class | image).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Generative models have several advantages and disadvantages with respect to discriminative ones. In a quick and dirty summary, they allow us to better understand if/how we are capturing the real-world distribution through our dataset, but they are harder to understand, implement, and scale-up to large quantities of data.&lt;/p&gt;

&lt;p&gt;However, it is important to be aware of their existence. Today we will be discussing probably the simplest instance of a generative model for classification, namely &lt;strong&gt;Naive Bayes&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;1-3-aside-working-from-spyder&#34;&gt;1.3 - &lt;strong&gt;Aside: Working from Spyder&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;At some point you may feel uncomfortable with the room that notebooks leave for playing with your code or organizing it. For instance, you will have noticed that there is lots of redundant code above.&lt;/p&gt;

&lt;p&gt;At this link, I am attaching a series of files meant to reproduce the above example in Spyder. Please download them to your working folder, open them, and observe how the different &lt;code&gt;import&lt;/code&gt; statements interact among them. You do not need to understand all this code does, but it is important that you understand the way it flows.&lt;/p&gt;

&lt;p&gt;By the way, you can also write code in a separate &lt;code&gt;.py&lt;/code&gt; file, import it, and use it from your notebook, as long as they both are in the same folder. See the example below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import daco_visualizations as dv
dv.plot_vertical_line(1,2,4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig4.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-4-quick-overview-of-scikit-learn&#34;&gt;1.4 - &lt;strong&gt;Quick overview of scikit-learn&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The main Python library for machine learning is called &lt;strong&gt;scikit-learn&lt;/strong&gt;, often abbreviated sklearn. We have already used a function belonging to sklearn above, namely &lt;code&gt;make_blobs()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Sklearn exposes to the user useful objects and functionalities to solve machine learning problems, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Classification&lt;/li&gt;
&lt;li&gt;Regression&lt;/li&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;Dimensionality Reduction&lt;/li&gt;
&lt;li&gt;Model Selection&lt;/li&gt;
&lt;li&gt;Preprocessing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sklearn even brings a module with some datasets with it that can be used to illustrate machine learning problems. There are specific functions to load those datasets available. Let us load the famous Iris flower dataset, consisting of $50$ observations of three species of Iris, namely Iris setosa, Iris virginica and Iris versicolor. In this case, four features are measured from each sample: length and the width of the sepals and petals. The label in this case is the kind of iris flower, and it consists of a 3-class classification problem.&lt;/p&gt;

&lt;p&gt;Let us load the dataset, keeping only the first two classes for demonstration purposes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import datasets
import numpy as np
iris = datasets.load_iris()

subset = np.logical_or(iris.target == 0, iris.target == 1)

X = iris.data[subset]
y = iris.target[subset]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;(X,y)&lt;/code&gt; subdataset has now two classes and 50 elements on each one. Each example is described by 50 features.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(X[y==0].shape)
print(X[y==2].shape)
print(X[0,:], y[0])
print(X[60,:], y[60])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How does sklearn exposes its tools? Through interfaces with the user. The main ones are:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;score = obj.score(data)    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Estimator&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;estimator = obj.fit(data, targets) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Predictor&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prediction = obj.predict(data) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new_data = obj.transform(data) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For instance, given the above dataset, you can instantiate a model.
The following is an example of a linear classifier, that we will discuss in class next week, but for the moment focus only on the &amp;ldquo;computational&amp;rdquo; side of this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

print(type(model))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now use the &lt;code&gt;Estimator&lt;/code&gt; interface to learn the parameters that best fit a model to your data and labels:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.fit(X, y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With a fitted model, you can for instance check if it is performing adequately (in this case the solution was perfect):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.score(X, y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the model has been fitted, you can also use it to predict labels on new data.
You can even do this for a batch of new examples in one single line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.predict([[5,4,2,1], [5,2,4,1]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will be covering most of the other functionalities that sklearn gives you in the remaining lectures.
For those who are curious, the documentation (which is &lt;strong&gt;really&lt;/strong&gt; nice and full of examples) can be found &lt;a href=&#34;http://scikit-learn.org/stable/documentation.htmls&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;2-bayesian-classification-a-bit-of-theory&#34;&gt;2.- &lt;strong&gt;Bayesian Classification: a Bit of Theory&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;You have already seen in the theoretical lectures Bayes&amp;rsquo; formula:&lt;/p&gt;

&lt;p&gt;$$\mathcal{P}(A \ | \ B) = \frac{\mathcal{P}(B \ | \ A) \cdot \mathcal{P}(A)}{\mathcal{P}(B)} \ \ \ \ \ (1)$$&lt;/p&gt;

&lt;p&gt;Which you should interpret as the posterior probability of $A$ given $B$ being proportional to the posterior probability of $B$ given $A$ multiplied by the probability of the prior $A$.
Or in plain English, that your knowledge on a given event $\mathcal{P}(A)$ is updated after performing an experiment $B$ with a multiplier $\mathcal{P}(B \ | \ A)$, becoming $\mathcal{P}(A \ | \ B)$.&lt;/p&gt;

&lt;p&gt;But, how do we use Bayes&amp;rsquo; rule in practice to design a Bayesian classifier?&lt;/p&gt;

&lt;p&gt;Consider  a classification problem with $K$ possible classes, in which the data has been represented by $\mathbf{x} = (x_1,x_2, \ldots, x_N)$, and we denote the classes as $\omega_k$, $1\leq k\leq K$. In the absence of training data, the simplest decision rule for classifying a new sample would be to pick the class $\hat{\omega}$ for which the prior probability (which you have from the beginning and states your opinion of the world before you see the training data):&lt;/p&gt;

&lt;p&gt;$$\hat{\omega} (\mathbf{x}) = \underset{k}{\operatorname{argmax}} \left[\mathcal{P}(\omega_k) \right]_{k=1}^K$$&lt;/p&gt;

&lt;p&gt;For instance, if you know that a particular type of cancer has an incidence of $0,1\%$ in the population, when a new patient arrives, you could tell her that she has cancer with probability $p=0.001$. This is a quite poor diagnosing strategy.&lt;/p&gt;

&lt;p&gt;However, our training data gives us a way to model $\mathcal{P}(\mathbf{x} \ | \ \omega_k)$, since we know the labels associated to each example. We will see below the way we build that model from the available data.&lt;/p&gt;

&lt;p&gt;But for now, consider in this context Bayes&amp;rsquo; rule: it gives us a way of updating our knowledge. Formula (1) is re-written now as:&lt;/p&gt;

&lt;p&gt;$$\mathcal{P}(\omega_k | \mathbf{x}) = \frac{\mathcal{P}(\mathbf{x} | \omega_k) \times \mathcal{P}(\omega_k)}{\mathcal{P(\mathbf{x})}} \ \ \ \ \ (2)$$&lt;/p&gt;

&lt;p&gt;It is the left-hand side of this formula, i.e. the &lt;strong&gt;Posterior probability&lt;/strong&gt;, that we need in order to classify new data into one of our classes. What do we have to build this?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Likelihood&lt;/strong&gt; $\mathcal{P}(\mathbf{x} | \omega_k)$, that we will extract from the training data.&lt;/li&gt;
&lt;li&gt;Our &lt;strong&gt;Prior&lt;/strong&gt; $\mathcal{P}(\mathbf{\omega_k})$ of how is data actually distributed into our classes in our experience.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Evidence&lt;/strong&gt; $\mathcal{P(\mathbf{x})}$, a term that you can consider just a simple scaling factor to ensure that the posterior probability adds up to $1$ and thus it is a correct probability distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this context, there is a simple decision rule, called MAP (&lt;strong&gt;Maximum A Posteriori probability&lt;/strong&gt;), that allows us to classify new data: assign to $\mathbf{x}$ the class $\hat{\omega}_{MAP}$ that receives the greatest amount of posterior probability $\mathcal{P}(\omega_k | \mathbf{x})$ according to $(2)$. Mathematically, this is formulated as:&lt;/p&gt;

&lt;p&gt;$$\hat{\omega}_{MAP} (\mathbf{x}) = \underset{k}{\operatorname{argmax}}  \left[\mathcal{P}(\omega_k|\mathbf{x})\right]_{k=1}^K = \underset{k}{\operatorname{argmax}} \left[\mathcal{P}(\mathbf{x}|\omega_k) \cdot  \mathcal{P}(\omega_k)\right]_{k=1}^K,$$&lt;/p&gt;

&lt;p&gt;where we have already dropped the evidence in the denominator, since we are interested in finding the a-posteriori maximizing class, and $\mathcal{P}(\mathbf{x})$ will be a factor affecting equally all classes (it does not depend on $k$).&lt;/p&gt;

&lt;p&gt;Since it is easier to maximize a sum than to maximize a product, we prefer to split the above MAP solution into:&lt;/p&gt;

&lt;p&gt;$$\hat{\omega}_{MAP} (\mathbf{x}) = \underset{k}{\operatorname{argmax}} \left[\log(\mathcal{P}(\mathbf{x}|\omega_k)) + \log(\mathcal{P}(\omega_k))\right]_{k=1}^K \ \ \ \ \ (3)$$&lt;/p&gt;

&lt;p&gt;And now it comes the magic: we are going to assume that the distribution of each class $k$ can be modeled through a Gaussian distribution parametrized by a mean and a standard deviation given by $(\mu_k, \Sigma_k)$:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\mathcal{P}(\mathbf{x} | \omega_k) &amp;amp;\sim \mathcal{N}(\mathbf{x} \ | \ \mu_k, \Sigma_k)
= \frac{\displaystyle 1}{\displaystyle (2\pi)^{N/2}} \cdot \frac{\displaystyle 1}{\displaystyle |\Sigma_k|^{N/2}} \cdot  \mathrm{exp}\left(\displaystyle \frac{-1}{2} \cdot (\mathbf{x} - \mu_k)^T \cdot \Sigma_k^{-1} \cdot (\mathbf{x}-\mu_k)\right), \ \ \ 1\leq k \leq K,
\end{align} \ \ \ \ \ (4)
$$&lt;/p&gt;

&lt;p&gt;where $|\cdot|$ represents the determinant of a matrix. Please note that $N$ is the dimension of our problem, in terms of the number of features that describe the data, i.e. $\mathbf{x} = (x_1,\ldots,x_N)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aside: Normal Distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you are familiar with the general formulation of a Gaussian (normal) distribution in many dimensions, you can skip the following paragraphs.&lt;/p&gt;

&lt;p&gt;The above multi-dimensional distribution has an easy interpretation, as usual in maths, once you look at particular cases in low dimensions.
This allows you to map the new ideas to your previous knowledge (Bayes rule attacks again?).
Recall that a Normal distribution in 1D is defined as:&lt;/p&gt;

&lt;p&gt;$$
\mathcal{N}(x \ | \ \mu, \sigma) = \frac{\displaystyle 1}{\displaystyle \sqrt{2\pi\sigma}} \cdot  \mathrm{exp} \left(\displaystyle -\frac{(x-\mu)^2}{2\sigma^2}\right)
$$&lt;/p&gt;

&lt;p&gt;Which can be rewritten as:
$$
\mathcal{N}(x \ | \ \mu, \sigma) = \frac{1}{(2\pi)^{1 / 2}} \frac{1}{\sigma^{1 / 2}} \cdot
\mathrm{exp} \left(\displaystyle \frac{-1}{2} \cdot (x-\mu) \cdot \frac{1}{\sigma^2}\cdot (x-\mu)\right) \ \ \ \ \ (5)
$$
And here, $x, \mu, \sigma$ are simply real numbers.&lt;/p&gt;

&lt;p&gt;But what if you want to model random variables that depend on two numbers, i.e., how can we define a bivariate normal distribution?
In this case, we would have $\mathbf{x} = (x_1, x_2)$, $\mu = (\mu_1, \mu_2)$ and the standard deviations $(\sigma_1, \sigma_2)$ are represented into the covariance matrix, that captures the correlation between both variables:&lt;/p&gt;

&lt;p&gt;$$\Sigma=\begin{pmatrix}
\sigma_{1}^2 &amp;amp; \sigma_1\sigma_2 \\
\sigma_1\sigma_2 &amp;amp; \sigma_{2}^2
\end{pmatrix},$$&lt;/p&gt;

&lt;p&gt;with $|\Sigma|$ representing its determinant, which is another real number.&lt;/p&gt;

&lt;p&gt;If you stare a while to formula (5), comparing it with (4), you should now see how and why one generalizes the other. If you still cannot see it clearly, I encourage you to replace $\mathbf{x}, \mu, \Sigma$ by actual values and do some paper and pen algebra.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Let us move forward in our goal to build a model that can classify new data.
Remember the simple logarithmic rules:&lt;/p&gt;

&lt;p&gt;$$\log(a\cdot b) = \log(a) + \log(b), \ \ \log(a^b) = b\cdot\log(a), \ \ \log(\exp(a))=a.$$&lt;/p&gt;

&lt;p&gt;Inserting (4) into (3), we obtain:&lt;/p&gt;

&lt;p&gt;$$ \hat{\omega}_{MAP} (\mathbf{x}) = \underset{k}{\operatorname{argmax}}\left(\frac{-1}{2} (\mathbf{x}-\mu_k)\Sigma_k^{-1}(\mathbf{x}-\mu_k) - \frac{N}{2}\log(2\pi) - \frac{1}{2} \log(|\Sigma_k|) + \log(\mathcal{P}(\omega_k))\right) \ \ \ \ \ (6)$$&lt;/p&gt;

&lt;p&gt;The MAP decision rule tells us to select the class $k$ maximizing the above equation. This means that we can safely ignore the term in (6) that does not depend on $k$, since it is simply an added constant that won&amp;rsquo;t affect who is the &amp;ldquo;maximizating&amp;rdquo; class:&lt;/p&gt;

&lt;p&gt;$$ \hat{\omega}_{MAP} (\mathbf{x}) = \underset{k}{\operatorname{argmax}}\left(\frac{-1}{2} (\mathbf{x}-\mu_k)\Sigma_k^{-1}(\mathbf{x}-\mu_k) - \frac{1}{2} \log(|\Sigma_k|) + \log(\mathcal{P}(\omega_k))\right) \ \ \ \ \ (7)$$&lt;/p&gt;

&lt;p&gt;This formula already gives us a classifier. We would estimate $\mu_k$ and $\Sigma$ by computing the different mean and standard deviations of each class &lt;strong&gt;on our training data&lt;/strong&gt;, and then insert them into (7).&lt;/p&gt;

&lt;p&gt;There are however further simplifying assumptions that we can make. We can assume that all the different classes are spread in the same away around their mean values, i.e. $\Sigma_k = \Sigma \ \forall k$. Then, the MAP Bayesian classifier is simplifed as:&lt;/p&gt;

&lt;p&gt;$$ \hat{\omega}_{MAP} (\mathbf{x}) = \underset{k}{\operatorname{argmax}}\left(\frac{-1}{2} (\mathbf{x}-\mu_k)\Sigma^{-1}(\mathbf{x}-\mu_k) + \log(\mathcal{P}(\omega_k))\right)$$&lt;/p&gt;

&lt;p&gt;Yet a more restrictive assumption is to suppose that there is independence between the different features describing the data $\mathbf{x}$. In this case the covariance matrix is diagonal, $\Sigma_k = \sigma^2 \mathrm{Id}$, hence $|\Sigma_k| = \sigma^2$, and the classifier is even more simple:&lt;/p&gt;

&lt;p&gt;$$ \hat{\omega}_{MAP} (\mathbf{x}) = \underset{k}{\operatorname{argmax}}\left(\frac{-1}{2\sigma^2} (\mathbf{x}-\mu_k)(\mathbf{x}-\mu_k) + \log(\mathcal{P}(\omega_k))\right) \ \ \ \ \ (8)$$&lt;/p&gt;

&lt;p&gt;The term appearing in the right part of equation (X) is a particularly relevant mathematical entity called &lt;strong&gt;Mahalanobis distance&lt;/strong&gt;. Let us analyze it.&lt;/p&gt;

&lt;p&gt;Consider two $N$-dimensional points $\mathbf{x} = (x_1, \ldots, x_N)^T$ $\mathbf{y} = (y_1, \ldots, y_N)^T$. The standard Euclidean distance between them is given by:&lt;/p&gt;

&lt;p&gt;$$
\mathrm{dist}(\mathbf{x}, \mathbf{y}) = | \mathbf{x} - \mathbf{y} |_2 = \sqrt{ (x_1 - y_1)^2 + \ldots + (x_N - y_N)^2}
$$&lt;/p&gt;

&lt;p&gt;Notice that this expression can be related to the usual vector product operation as follows:&lt;/p&gt;

&lt;p&gt;$$
\mathrm{dist}(\mathbf{x}, \mathbf{y})^2 = \left(\mathbf{x}-\mathbf{y}\right)^T \cdot \left(\mathbf{x}-\mathbf{y}\right)
$$&lt;/p&gt;

&lt;p&gt;The Mahalanobis distance between those to vectors is a generalization of the squared Euclidean distance:&lt;/p&gt;

&lt;p&gt;$$
\mathrm{dist}_{Mahalanobis}(\mathbf{x}, \mathbf{y}) = \left(\mathbf{x}-\mathbf{y}\right)^T \cdot A \cdot\left(\mathbf{x}-\mathbf{y}\right),
$$&lt;/p&gt;

&lt;p&gt;where we have introduced an $N\times N$ matrix in between both products. Note that if $A$ is the identity, both distances measure the same. However, if $A$ is a different matrix, interesting things can happen!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Let us define two bivariate normal distributions with means $\mu_1=(0,0)^T$ and $\mu_2=(1,1)^T$. Consider also that the covariance matrixes for each distribution are given by:&lt;/p&gt;

&lt;p&gt;$$\Sigma_1 = \Sigma_2 =
\begin{pmatrix}
\sigma_1^2 &amp;amp; 0 \\
0 &amp;amp; \sigma_2^2
\end{pmatrix}=
\begin{pmatrix}
0.5 &amp;amp; 0 \\
0 &amp;amp; 2
\end{pmatrix}.$$&lt;/p&gt;

&lt;p&gt;We can plot their contours with a code similar to the one we used in the introduction:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mu_1 = np.array([0,0])
mu_2 = np.array([1,1])

sigma_1 = 0.5
sigma_2 = 2

sigma = np.array([[sigma_1,0],[0,sigma_2]])

rv = multivariate_normal(mu_1, sigma) # normal distribution around (mu_1)
rv_2 = multivariate_normal(mu_2, sigma) # normal distribution around (mu_2)

x_grid, y_grid = np.mgrid[-3 : 3: 0.1, -3 : 4: 0.1]
pos = np.empty(x_grid.shape + (2,))
pos[:, :, 0] = x_grid; pos[:, :, 1] = y_grid

plt.contour(x_grid, y_grid, rv.pdf(pos), linewidths=0.5, colors = &#39;red&#39;);
plt.contour(x_grid, y_grid,rv_2.pdf(pos), linewidths=0.5, colors = &#39;green&#39;);
plt.plot(mu_1[0], mu_1[1], marker=&#39;o&#39;, markersize=10, color=&#39;red&#39;);
plt.plot(mu_2[0], mu_2[1], marker=&#39;o&#39;, markersize=10, color=&#39;green&#39;);
plt.plot(1, 0, marker=&#39;o&#39;, markersize=10, color=&#39;blue&#39;);
plt.axes().set_aspect(&#39;equal&#39;, &#39;datalim&#39;) # This line here guarantees 1:1 aspect-ratio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig5.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let us compute the Euclidean distance between each of the two distribution centers and the point $(0,1)^T$, plotted in blue in the previous snippet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def squared_dist(x,y):
    return np.sqrt((x-y).dot(x-y))

new_point = np.array([0,1])
print(squared_dist(mu_1,new_point))
print(squared_dist(mu_2,new_point))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, implement and compute the Mahalanobis distance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def mahalanobis_dist(x,y,A):
    return (x-y).dot(A).dot(x-y)

print(mahalanobis_dist(mu_1, new_point, sigma))
print(mahalanobis_dist(mu_2, new_point, sigma))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea is that according to the Euclidean distance, both mean distributions are equally far away from the blue point, while the Mahalanobis formulation weighs that distance by considering the &amp;ldquo;shape&amp;rdquo; of the distributions, enconded in the covariance matrix: the second (green) distribution is closer to the point. This implies that the second distribution is more likely to have generated the blue sample.&lt;/p&gt;

&lt;p&gt;This simple idea is actually what a Naive Bayes classifier implements in the case of the last simplification above:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Given training data, we model each class distribution via a normal distribution.&lt;/li&gt;
&lt;li&gt;Given a new point, we compare its distance with regard to each distributions with the Mahalanobis distance&lt;/li&gt;
&lt;li&gt;The closest distribution is the one most likely to have generated the new point, and we assign that class to it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note however that the Bayesian formulation also allows us to introduce our prior experience in the model via the term $\log(\mathcal{P}(\omega_k))$ from eq. (8).&lt;/p&gt;

&lt;h1 id=&#34;3-naive-bayes-classifier-implementation&#34;&gt;3.- &lt;strong&gt;Naive Bayes Classifier Implementation&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Let us see how can we implement the above ideas for classification tasks.&lt;/p&gt;

&lt;h2 id=&#34;3-1-from-scratch&#34;&gt;3.1 - &lt;strong&gt;From scratch&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;To experiment with real data, we are going to use the Pima Indian dataset, provided by the UCI Machine Learning repository, and described &lt;a href=&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. The dataset contains $768$ observations (from indian patients) of $8$ different features, and the task is to try to predict if the patient will suffer from diabets. This means we face a binary classification problem. The observed features are all real or integer variables:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of times pregnant&lt;/li&gt;
&lt;li&gt;Plasma glucose concentration a 2 hours in an oral glucose tolerance test&lt;/li&gt;
&lt;li&gt;Diastolic blood pressure (mm Hg)&lt;/li&gt;
&lt;li&gt;Triceps skin fold thickness (mm)&lt;/li&gt;
&lt;li&gt;2-Hour serum insulin (mu U/ml)&lt;/li&gt;
&lt;li&gt;Body mass index (weight in kg/(height in m)^2)&lt;/li&gt;
&lt;li&gt;Diabetes pedigree function&lt;/li&gt;
&lt;li&gt;Age (years)&lt;/li&gt;
&lt;li&gt;Class variable (0 or 1)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We are going to use &lt;code&gt;pandas&lt;/code&gt; for importing the data directly from the internet. &lt;code&gt;pandas&lt;/code&gt; is another Python library, useful for data analysis, but you do not need to worry about it right now, we will immediately store the data into a &lt;code&gt;numpy&lt;/code&gt; array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data&#39;,
                 header=None)
df.columns = [&#39;#pregnant&#39;, &#39;glucose&#39;, &#39;blood_pressure&#39;, &#39;thickness&#39;, &#39;insulin&#39;, &#39;body_mass_idx&#39;, 
              &#39;pedigree&#39;, &#39;age&#39;, &#39;label&#39;]
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that &lt;code&gt;pandas&lt;/code&gt; contains useful functions to explore your data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

df.describe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let us store the features and the labels in separate &lt;code&gt;numpy&lt;/code&gt; arrays:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = np.array(df)
print(df.shape)

features = dataset[:, :8]
labels = dataset[:, -1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are going to use a simplified set of features to illustrate more easily how to implement a Naive Bayes classifier.
Let us select for instance age and body mass index:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_reduced = df[[&#39;body_mass_idx&#39;,&#39;age&#39;]]
features_reduced = np.array(df_reduced)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To model each class, we rely on their mean and standard deviation, and make the assumption that variables are independent. This is in fact why this algorithm is called &amp;ldquo;Naive&amp;rdquo;. If features are correlated, this assumption may harm the performance.
An example of potentially correlated features is, for instance, age and number of times pregnant.&lt;/p&gt;

&lt;p&gt;First we need to separate the dataset into each of the classes we want to model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;features_healthy = features_reduced[labels==0]
features_diabetes = features_reduced[labels==1]
mean_healthy = np.mean(features_healthy, axis=0)
mean_diabetes = np.mean(features_diabetes, axis=0)

std_healthy = np.std(features_healthy, axis=0)
std_diabetes = np.std(features_diabetes, axis=0)

std_both = (std_healthy + std_diabetes)/2

sigma = np.array([[std_both[0], 0], [0, std_both[0]]])
sigma_diabetes = np.array([[std_both[0], 0], [0, std_both[1]]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time to build (and plot) our two Gaussian models:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;healthy_model = multivariate_normal(mean_healthy, std_both) 
diabetes_model = multivariate_normal(mean_diabetes, std_both)

x_grid, y_grid = np.mgrid[23 : 42: 0.1, 23 : 45: 0.1]
pos = np.empty(x_grid.shape + (2,))
pos[:, :, 0] = x_grid; pos[:, :, 1] = y_grid

plt.contour(x_grid, y_grid, healthy_model.pdf(pos), linewidths=0.5, colors = &#39;green&#39;);
plt.contour(x_grid, y_grid, diabetes_model.pdf(pos), linewidths=0.5, colors = &#39;red&#39;,);
plt.plot(mean_healthy[0], mean_healthy[1], marker=&#39;o&#39;, markersize=10, color=&#39;green&#39;, label = &#39;Healthy&#39;);
plt.plot(mean_diabetes[0], mean_diabetes[1], marker=&#39;o&#39;, markersize=10, color=&#39;red&#39;, label = &#39;Diabetes&#39;);
plt.axes().set_aspect(&#39;equal&#39;, &#39;datalim&#39;) # This line here guarantees a 1:1 aspect-ratio
plt.legend()
plt.xlabel(&#39;Body Mass Index&#39;);
plt.ylabel(&#39;Age&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig6.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, if I am a patient with a body mass index of 40, and 33 years old, which decision will make this classifier?
And if I am 25 years old? Think first, or try to predict the result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def print_diagnosis(body_mass, age):
    distance_diabetes = mahalanobis_dist(mean_diabetes, [body_mass, age], sigma)
    distance_healthy = mahalanobis_dist(mean_healthy, [body_mass, age], sigma)
    if distance_diabetes &amp;gt; distance_healthy:
        print(&#39;Diagnostic for body mass&#39;, body_mass, &#39;and age&#39;, age, &#39;is: healthy&#39;)
    else:
        print(&#39;Diagnostic for body mass&#39;, body_mass, &#39;and age&#39;, age, &#39;is: diabetes&#39;)
        
print_diagnosis(40,33)
print_diagnosis(40,25)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-using-scikit-learn&#34;&gt;3.2 - &lt;strong&gt;Using scikit-learn&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The documentation for Naive Bayes in sklearn can be reached &lt;a href=&#34;http://scikit-learn.org/stable/modules/naive_bayes.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for the Gaussian case.&lt;/p&gt;

&lt;p&gt;The simplicity with which we can apply the Naive Bayes classifier to our data with sk-learn speaks by itself:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(features_reduced, labels)

print(clf.predict([40, 33]))
print(clf.predict([40, 25]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read (and fix)the warning!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.naive_bayes import GaussianNB
data= np.array([40, 33]).reshape(1,-1)
print(clf.predict(data))

data= np.array([40, 25]).reshape(1,-1)
print(clf.predict(data))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Observe also how easily we can generalize to dealing with all the features with little effort:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clf_all = GaussianNB()
clf_all.fit(features, labels)

print(clf.predict(np.array([40, 33]).reshape(1,-1)))
print(clf.predict(np.array([40, 25]).reshape(1,-1)))
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-linear-regression-a-tiny-bit-of-theory&#34;&gt;4.- &lt;strong&gt;Linear Regression: a (tiny) bit of theory&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;In order to give an example of a problem different than classification, we are going to use Linear Regression.
The idea of Linear Regression is simply to find the linear function that best explains our labeled data.
Hence, in the one-dimensional case, we will be finding simply a line; in case we have two features, this will be a plane, etc.&lt;/p&gt;

&lt;p&gt;Regardless of the dimension, every linear space can be described as:&lt;/p&gt;

&lt;p&gt;$$L(\mathbf{x},\theta) = \theta_0 + \theta_1 \cdot x_1 + \ldots +\theta_N x_N$$&lt;/p&gt;

&lt;p&gt;and the set of parameters $\theta=(\theta_0, \theta_1, \ldots, \theta_N)$ are the numbers that specify the subspace. For instance, in the one-dimensional case, we would have:&lt;/p&gt;

&lt;p&gt;$$L(\mathbf{x},\theta) = \theta_0 + \theta_1 \cdot x_1$$&lt;/p&gt;

&lt;p&gt;which is the equation of a straight line.&lt;/p&gt;

&lt;p&gt;We could then have a dataset for linear regression that has the following appearance:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;features = np.array([1.7, 2.4, 3.5, 4.2, 5.7, 6.9, 8.2, 9.5])
targets = np.array([20.2, 35.3, 50.4, 51.2, 75.2, 62.7, 87.3, 90.1])

import matplotlib.pyplot as plt
plt.scatter(features, targets, s=100);
plt.xlabel(&#39;Globules Concentration&#39;)
plt.ylabel(&#39;Blood Pressure&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig7.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finding a linear model that solves this problem would imply specifying $\theta_0, \theta_1$ in the above equation.
For instance we can specify $\theta_0 = 20$, $\theta_1 = 1$ and see if it fits our data properly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta_0 = 20
theta_1 = 9
x = np.linspace(0, 10, 100)
y = theta_0 + theta_1*x
plt.plot(x,y, &#39;r--&#39;, lw = 2, label = &#39;linear regression fit&#39;)
plt.scatter(features, targets, s=100);
plt.xlabel(&#39;Globules Concentration&#39;)
plt.ylabel(&#39;Blood Pressure&#39;)
plt.legend()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig8.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you trust in your model, you can now use it to predict blood pressure out of, for instance, a patient with globules concentration of $x=5$:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_patient_concentration = 5.0
predicted_blood_pressure = theta_0 + theta_1*new_patient_concentration
print(predicted_blood_pressure)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As mentioned above, we are going to discuss deeply this kind of models both for classification and regression in the theoretical lectures.
However, it is useful for you to see how does sk-learn handle this kind of problems, which we do below.&lt;/p&gt;

&lt;h1 id=&#34;5-linear-regression-using-scikit-learn&#34;&gt;5.- &lt;strong&gt;Linear Regression Using scikit-learn&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;As you may be expecting, you can easily accomplish this task with sk-learn:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LinearRegression

# Create Model
model = LinearRegression()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After instantiating a linear regressor, we can fit it in the same way as we fit a classifier:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit Model
model.fit(features.reshape(-1, 1), targets)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we are now ready for making new predictions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit Model
model.predict(5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can compare the fit computed by sk-learn with the one we manually computed above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fit Model
predicted_by_sklearn = model.predict(x.reshape(-1,1))
plt.plot(x,y, &#39;r--&#39;, lw = 2, label = &#39;Manual LR&#39;)
plt.plot(x,predicted_by_sklearn, &#39;g--&#39;, lw = 2, label = &#39;sklearn LR&#39;)
plt.scatter(features, targets, s=100);
plt.xlabel(&#39;Globules Concentration&#39;)
plt.ylabel(&#39;Blood Pressure&#39;)
plt.legend()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://agaldran.github.io/img/daco17/pl2_fig9.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;6-homework&#34;&gt;6.- &lt;strong&gt;Homework&lt;/strong&gt;&lt;/h1&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;6-homework-1&#34;&gt;6. &lt;strong&gt;Homework&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&#34;6-1-graded-homework-training-and-testing-data&#34;&gt;6.1 - Graded Homework: Training and Testing data.&lt;/h2&gt;

&lt;h3 id=&#34;first-part&#34;&gt;First part&lt;/h3&gt;

&lt;p&gt;It is important to test your trained models on new data that has not been used for training. Otherwise you could be overly optimistic on its performance. For that, we typically save some training data and use them in the end only. In the same way as we refer to the data used to learn a model as &lt;strong&gt;training set&lt;/strong&gt;, we refer to the independent data you hold apart from the begining as &lt;strong&gt;test set&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Sk-learn provides functionality for easily doing this operation. Look up in google or wherever for this sk-learn method, and apply it to split the full Pima Indian dataset from section 3.1 into a training set ($75\%$ of the data) and a test set. Train a Naive Bayes classifier on the training set, and then predict the appearence of diabetes on every element of the test set. Store your predictions in a &lt;code&gt;numpy&lt;/code&gt; array with two columns, allocating in the first one the predictions.&lt;/p&gt;

&lt;p&gt;Print your predictions on the test set together with the correct labels using the function provided below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def print_predictions_and_labels(array_preds_labels):
    for predicted_pair in array_preds_labels:
        prediction = predicted_pair[0]
        label = predicted_pair[1]
        print(&#39;Prediction&#39;, prediction, &#39;Label&#39;, label)

preds_and_labels = np.array([[1,1],[0,1],[1,0]])
print_predictions_and_labels (preds_and_labels)       
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;second-part&#34;&gt;Second part&lt;/h3&gt;

&lt;p&gt;The simplest way you can evaluate a binary classifier is by means of accuracy, which is defined as the number of data points (in your test set!) that were correctly classified over number of total data points.&lt;/p&gt;

&lt;p&gt;Again, sklearn comes to help you. Look for a function (contained in &lt;code&gt;sklearn.metrics&lt;/code&gt;) that can give you the accuracy score your classifier achieves. Read the documentation, undestand how is that function used, and use it to print the accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is an alternative to that function. Every fitted classifier in &lt;code&gt;sklearn&lt;/code&gt; has a method of its own that you can use to output accuracy on a separate test set.
This method is named &lt;code&gt;score&lt;/code&gt;. Use it to find out your accuracy in the test set, and verify that it is the same as in the above case.&lt;/p&gt;

&lt;h2 id=&#34;6-2-graded-homework-simple-linear-regression&#34;&gt;6.2 - Graded Homework: Simple Linear Regression&lt;/h2&gt;

&lt;h3 id=&#34;first-part-1&#34;&gt;First part&lt;/h3&gt;

&lt;p&gt;We now consider a different dataset, but related also to diabetes. In this case we want to do linear regression. The dataset is described &lt;a href=&#34;https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/descr/diabetes.rst&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, and contains ten variables (features): age, sex, body mass index, average blood pressure, and six blood serum measurements. These were extracted from a set of $442$ diabetes patients, and the target prediction is given by certain (unspecified) quantitative measure of disease progression one year after baseline.&lt;/p&gt;

&lt;p&gt;This dataset comes already with sklearn, so you can load it from the &lt;code&gt;sklearn.datasets&lt;/code&gt; module. We can load the dataset into a tuple containing two numpy arrays as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;diabetes_dataset = datasets.load_diabetes(return_X_y=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following two lines will fit a linear regressor to this dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = LinearRegression()
model.fit(diabetes_dataset[0], diabetes_dataset[1])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your goal now is to fit again the regressor, but this time split first the dataset into a training and a test set, and train the model &lt;strong&gt;only&lt;/strong&gt; on the training data, reserving $30\%$ of your test set apart.&lt;/p&gt;

&lt;h3 id=&#34;second-part-1&#34;&gt;Second part&lt;/h3&gt;

&lt;p&gt;Evaluating regression models requires different error measures than accuracy, since we do not have now an idea of well/wrong classified. In this case, you can observe the mean square error in your test set, defined as you may have expected:&lt;/p&gt;

&lt;p&gt;$$MSE = \sum_{i=1}^N(\mathrm{target}_i - \mathrm{prediction}_i)^2$$&lt;/p&gt;

&lt;p&gt;But to begin with, you need to compute the predictions of your model on the test data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;diabetes_test_predictions = model2.predict(X_test)
type(diabetes_test_predictions)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can use the MSE error provided for you by the &lt;code&gt;sklearn.metrics&lt;/code&gt; module under the name &lt;code&gt;mean_squared_error&lt;/code&gt;.
Import it and use it to compute the error. Note that you will need to feed it the predictions and the actual values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, diabetes_test_predictions)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, the precise error value does not help too much on itself, since we do not have a reference value to compare with.
However, notice that it can be useful to compare with other regression models.&lt;/p&gt;

&lt;h2 id=&#34;6-3-ungraded-homework-k-nearest-neighbor-classification&#34;&gt;6.3 - Ungraded Homework: k-nearest neighbor classification&lt;/h2&gt;

&lt;p&gt;k-nearest neighbor is a very intuitive classifier that works as follows: Suppose you have a dataset with examples given by:&lt;/p&gt;

&lt;p&gt;$${\mathbf{x}^i, y_i} \ \ \mathrm{for} \ \ 1\leq i \leq m,$$&lt;/p&gt;

&lt;p&gt;described by $n$ features $\mathbf{x}^i = (x^i_0,x^i_1, \ldots, x^i_n)$, with labels $y_i$.&lt;/p&gt;

&lt;p&gt;Given a new example $\mathbf{x}^{new} = (x^{new}_0,x^{new}_1, \ldots, x^{new}_n)$, your model will assign to it a label by comparing the distance between $\mathbf{x}^{new}$ and all your training examples, and the decision is made depending on the label of the closest $k$ neighbors.&lt;/p&gt;

&lt;p&gt;In this case, we are going to fix $k=3$. So, if the lowest distances were given by, e.g., $\mathbf{x}^{12}$, $\mathbf{x}^{57}$, and $\mathbf{x}^{62}$, and you have that $y^{12} = 0, y^{57} = 1,  y^{62} = 1$, then the new sample receives a prediction of $y^{new}=1$.&lt;/p&gt;

&lt;p&gt;In order to implement k-nearest neighbors, you need a definition of &amp;ldquo;closest&amp;rdquo;, which means that you need a definition of distance between to $n$-dimensional points. We will be using the standard Euclidean distance, defined as the norm of the difference between the two points:&lt;/p&gt;

&lt;p&gt;$$\mathrm{dist}(\mathrm{p}, \mathrm{q})  = |\mathrm{p} - \mathrm{q}|_2 = \sqrt{(p_1-q_1)^2 + (p_2-q_2)^2 + \ldots (p_n-q_n)^2 + }$$&lt;/p&gt;

&lt;p&gt;This distance can be easily implemented, or you can use the &lt;code&gt;norm&lt;/code&gt; function from &lt;code&gt;numpy.linalg&lt;/code&gt; library:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def distance(x,y):
    return np.linalg.norm(x-y)

p = np.array([0,0])
q = np.array([2,0])

print(&#39;Distance between (0,0) and (2,0) is&#39;, distance(p,q))

p = np.array([0,0,0,0])
q = np.array([2,2,0,0])
print(&#39;Distance between (0,0) and (2,0) is&#39;, distance(p,q))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;first-part-implementation-from-scratch&#34;&gt;First part: Implementation from scratch&lt;/h3&gt;

&lt;p&gt;I am completely aware that this could be challenging if you are not too familiar with &lt;code&gt;numpy&lt;/code&gt;. However, one of the best ways to learn both coding and the inner mechanisms of machine learning models is to &lt;strong&gt;try&lt;/strong&gt; to implement them from scratch. No matter if your implementation is the most inefficient in the world, no matter if it is not even free of Python errors, I just want you to try implementing the idea described above. Submit &lt;strong&gt;whatever&lt;/strong&gt; your attempt to solve this is, I don&amp;rsquo;t mind if you feel what you do is not the best implementation in the world or even if you think you are ashamed of it. Please submit!&lt;/p&gt;

&lt;p&gt;We are going to use the same reduced version of the Iris dataset as in section 1.d. This is loaded for you again in the next code snippet. Try to implement code for classifying a new example given by $\mathbf{x}^{new} = (3.5,2.5,2.5,0.75)$.&lt;/p&gt;

&lt;p&gt;If you succeed implementing this, then split the training and test set (30$\%$ for the test set) and compute the accuracy error from all the examples in the test set.&lt;/p&gt;

&lt;h3 id=&#34;second-part-knn-with-sk-learn&#34;&gt;Second part: KNN with &lt;code&gt;sk-learn&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Now investigate how can you replicate the above, but this time with the sklearn library.&lt;/p&gt;

&lt;h1 id=&#34;7-sources-and-references&#34;&gt;7. &lt;strong&gt;Sources and References&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;[1] Theoretical lessons for DACO, in your moodle.&lt;/p&gt;

&lt;p&gt;[2] It could be useful to watch &lt;a href=&#34;https://www.youtube.com/watch?v=z5UQyCESW64&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; short video on generative models.&lt;/p&gt;

&lt;p&gt;[3] The second lecture of the Udacity&amp;rsquo;s online course Intro to Machine Learning, accessible &lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] You can find a nice overview and implementation of k-NN in &lt;a href=&#34;https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/&#34; target=&#34;_blank&#34;&gt;Kevin Zakka&amp;rsquo;s blog&lt;/a&gt;. Be aware that he is making more us of the &lt;code&gt;pandas&lt;/code&gt; library than we do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DACO - Theoretical Lecture 6a - Linear Models</title>
      <link>https://agaldran.github.io/post/17_daco_theory_lec_6a/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0100</pubDate>
      
      <guid>https://agaldran.github.io/post/17_daco_theory_lec_6a/</guid>
      <description>&lt;p&gt;Today&amp;rsquo;s lecture is split in two parts.&lt;/p&gt;

&lt;p&gt;In the first part, you will be introduced to a simple, but fundamental kind of machine learning models, Linear models.
We will understand how are they formulated, and how can they be solved numerically.
We will also see how can they be used for both regression and classification.&lt;/p&gt;

&lt;p&gt;The second part comprises an introduction to non-linear models, specifically to neural networks.
We will continue studying neural networks in the next lecture, but this week you will already understand what are they and how can they be used for classification tasks.
The material related to this second part is located &lt;a href=&#34;google.es&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is an overview of what we will be learning in this first hour:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Motivation and Goals. Linear Models for Supervised Classification and Regression&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Linear Regression Models&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Linear Regression for one variable&lt;/li&gt;
&lt;li&gt;Gradient descent for Linear Regression&lt;/li&gt;
&lt;li&gt;Linear Regression - A real-world example&lt;/li&gt;
&lt;li&gt;Linear Regression for more than one variable
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Logistic Regression Models (for classification!)&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First part&lt;/li&gt;
&lt;li&gt;Second part&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sources and References&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The same as in the practical lectures, I have also built a Jupyter notebook that contains all the information you will find below.
In addition, it will allow you to experiment with short pieces of code. You can access it &lt;a href=&#34;google.es&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>DACO - Theoretical Lecture 6b - Introduction to Neural Networks</title>
      <link>https://agaldran.github.io/post/17_daco_theory_lec_6b/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0100</pubDate>
      
      <guid>https://agaldran.github.io/post/17_daco_theory_lec_6b/</guid>
      <description>&lt;p&gt;Under construction.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&#34;https://metacademy.org/graphs/concepts/backpropagation&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for backpropagation.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
